{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's this TensorFlow business?\n",
    "\n",
    "You've written a lot of code in this assignment to provide a whole host of neural network functionality. Dropout, Batch Norm, and 2D convolutions are some of the workhorses of deep learning in computer vision. You've also worked hard to make your code efficient and vectorized.\n",
    "\n",
    "For the last part of this assignment, though, we're going to leave behind your beautiful codebase and instead migrate to one of two popular deep learning frameworks: in this instance, TensorFlow (or PyTorch, if you switch over to that notebook)\n",
    "\n",
    "#### What is it?\n",
    "TensorFlow is a system for executing computational graphs over Tensor objects, with native support for performing backpropogation for its Variables. In it, we work with Tensors which are n-dimensional arrays analogous to the numpy ndarray.\n",
    "\n",
    "#### Why?\n",
    "\n",
    "* Our code will now run on GPUs! Much faster training. Writing your own modules to run on GPUs is beyond the scope of this class, unfortunately.\n",
    "* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n",
    "* We want you to stand on the shoulders of giants, e.g., TensorFlow. This very excellent framework will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n",
    "* We want you to be exposed to the sort of deep learning code you might run into in academia or industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgement: This exercise is adapted from [Stanford CS231n](http://cs231n.stanford.edu/index.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will I learn TensorFlow?\n",
    "\n",
    "TensorFlow has many excellent tutorials available, including those from [Google themselves](https://www.tensorflow.org/get_started/get_started).\n",
    "\n",
    "Otherwise, this notebook will walk you through much of what you need to do to train models in TensorFlow. See the end of the notebook for some links to helpful tutorials if you want to learn more or need further clarification on topics that aren't fully explained here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from libs.tf_layers import *\n",
    "from libs.vis_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from libs.data_utils import load_CIFAR10\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'libs/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Model\n",
    "\n",
    "### Some useful utilities\n",
    "\n",
    ". Remember that our image data is initially N x H x W x C, where:\n",
    "* N is the number of datapoints (mini-batch size)\n",
    "* H is the height of each image in pixels\n",
    "* W is the height of each image in pixels\n",
    "* C is the number of channels (usually 3: R, G, B)\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, which needs spatial understanding of where the pixels are relative to each other. When we input image data into fully connected affine layers, however, we want each data example to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example model itself\n",
    "\n",
    "The first step to training your own model is defining its architecture.\n",
    "\n",
    "Here's an example of a convolutional neural network defined in TensorFlow -- try to understand what each line is doing, remembering that each layer is composed upon the previous layer. We haven't trained anything yet - that'll come next - for now, we want you to understand how everything gets set up. \n",
    "\n",
    "In that example, you see 2D convolutional layers (Conv2d), ReLU activations, and fully-connected layers (Linear). You also see the Hinge loss (multi-class SVM) function, and the SGD optimizer being used. \n",
    "\n",
    "Make sure you understand **why the parameters of the Linear layer are 5408 and 10**. You can refer to the material from [CS231n webpages](http://cs231n.github.io/convolutional-networks/).\n",
    "\n",
    "### TensorFlow Details\n",
    "In TensorFlow, much like in our previous notebooks, we'll first specifically initialize our variables, and then our network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def simple_model(X,y):\n",
    "    # define our weights (e.g. init_two_layer_convnet)\n",
    "    \n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[7, 7, 3, 32])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "    W1 = tf.get_variable(\"W1\", shape=[5408, 10])\n",
    "    b1 = tf.get_variable(\"b1\", shape=[10])\n",
    "\n",
    "    # define our graph (e.g. two_layer_convnet)\n",
    "    a1 = tf.nn.conv2d(X, Wconv1, strides=[1,2,2,1], padding='VALID') + bconv1\n",
    "    h1 = tf.nn.relu(a1)\n",
    "    h1_flat = tf.reshape(h1,[-1,5408])\n",
    "    y_out = tf.matmul(h1_flat,W1) + b1\n",
    "    return y_out\n",
    "\n",
    "y_out = simple_model(X,y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define SGD optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(2e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow supports many other layer types, loss functions, and optimizers - you will experiment with these next. Here's the official API documentation for these (if any of the parameters used above were unclear, this resource will also be helpful). \n",
    "\n",
    "* Layers, Activations, Loss functions : https://www.tensorflow.org/api_guides/python/nn\n",
    "* Optimizers: https://www.tensorflow.org/api_guides/python/train#Optimizers\n",
    "* BatchNorm: https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm. Note that there are few other implementations of batch normalization layers, e.g., [link 1](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm), [link 2](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on one epoch\n",
    "Define the function to train a model as following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    \n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    \n",
    "    training_now = (training is not None)\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss, correct_prediction, accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "\n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    # keep track of losses\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        # shuffle indicies\n",
    "        np.random.shuffle(train_indicies)\n",
    "        # keep track of accuracy\n",
    "        correct = 0\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            corr = np.array(corr).astype(np.float32)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        \n",
    "    if plot_losses:\n",
    "        plt.plot(losses)\n",
    "        plt.grid(True)\n",
    "        plt.title('Epoch {} Loss'.format(e+1))\n",
    "        plt.xlabel('minibatch number')\n",
    "        plt.ylabel('minibatch loss')\n",
    "        plt.show()\n",
    "            \n",
    "    return total_loss,total_correct,losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have defined a graph of operations above, in order to execute TensorFlow Graphs, by feeding them input data and computing the results, we first need to create a `tf.Session` object. A session encapsulates the control and state of the TensorFlow runtime. For more information, see the TensorFlow [Getting started](https://www.tensorflow.org/get_started/get_started) guide.\n",
    "\n",
    "Optionally we can also specify a device context such as `/cpu:0` or `/gpu:0`. For documentation on this behavior see [this TensorFlow guide](https://www.tensorflow.org/tutorials/using_gpu). Generally, if your machine has GPU available (with all required drivers) and you install Tensorflow GPU version, Tensorflow will automatically select GPU as the primary device. Otherwise, CPU will be selected as the primary device.\n",
    "\n",
    "You should see a validation loss of around 1 and an accuracy of 0.2 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 10.8 and accuracy of 0.031\n",
      "Iteration 100: with minibatch training loss = 2.32 and accuracy of 0.094\n",
      "Iteration 200: with minibatch training loss = 1.82 and accuracy of 0.14\n",
      "Iteration 300: with minibatch training loss = 1.38 and accuracy of 0.2\n",
      "Iteration 400: with minibatch training loss = 1.48 and accuracy of 0.17\n",
      "Iteration 500: with minibatch training loss = 1.2 and accuracy of 0.23\n",
      "Iteration 600: with minibatch training loss = 1.14 and accuracy of 0.16\n",
      "Iteration 700: with minibatch training loss = 1.09 and accuracy of 0.27\n",
      "Epoch 1, Overall loss = 1.52 and accuracy of 0.203\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Epoch 1, Overall loss = 1.02 and accuracy of 0.231\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    run_model(sess,y_out,mean_loss,X_train,y_train,1,64,100,train_step,plot_losses=True)\n",
    "    print('Validation')\n",
    "    run_model(sess,y_out,mean_loss,X_val,y_val,1,64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.9161997145348099&quot;).pbtxt = 'node {\\n  name: &quot;Placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 32\\n        }\\n        dim {\\n          size: 32\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Placeholder_1&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Placeholder_2&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\007\\\\000\\\\000\\\\000\\\\007\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000 \\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.05914847552776337\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.05914847552776337\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/max&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/mul&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 7\\n        }\\n        dim {\\n          size: 7\\n        }\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 32\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Wconv1&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Wconv1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 32\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.3061862289905548\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.3061862289905548\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/max&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/mul&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 32\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;bconv1&quot;\\n  input: &quot;bconv1/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;bconv1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot; \\\\025\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.03327791765332222\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.03327791765332222\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;W1/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;W1/Initializer/random_uniform/max&quot;\\n  input: &quot;W1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;W1/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;W1/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;W1/Initializer/random_uniform/mul&quot;\\n  input: &quot;W1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 5408\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;W1&quot;\\n  input: &quot;W1/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;W1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 10\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.547722578048706\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.547722578048706\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;b1/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;b1/Initializer/random_uniform/max&quot;\\n  input: &quot;b1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;b1/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;b1/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;b1/Initializer/random_uniform/mul&quot;\\n  input: &quot;b1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;b1&quot;\\n  input: &quot;b1/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;b1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Wconv1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Conv2D&quot;\\n  input: &quot;bconv1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377 \\\\025\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Relu&quot;\\n  input: &quot;Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;W1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;MatMul&quot;\\n  input: &quot;b1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;one_hot/on_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;one_hot/off_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;one_hot/depth&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 10\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;one_hot&quot;\\n  op: &quot;OneHot&quot;\\n  input: &quot;Placeholder_1&quot;\\n  input: &quot;one_hot/depth&quot;\\n  input: &quot;one_hot/on_value&quot;\\n  input: &quot;one_hot/off_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;TI&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: -1\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/ones_like/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;one_hot&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/ones_like/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/ones_like&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;hinge_loss/ones_like/Shape&quot;\\n  input: &quot;hinge_loss/ones_like/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/mul/x&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/mul/x&quot;\\n  input: &quot;one_hot&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hinge_loss/mul&quot;\\n  input: &quot;hinge_loss/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/Sub&quot;\\n  input: &quot;add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hinge_loss/ones_like&quot;\\n  input: &quot;hinge_loss/Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;hinge_loss/Sub_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/assert_broadcastable/weights&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/assert_broadcastable/weights/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/assert_broadcastable/weights/rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/assert_broadcastable/values/shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/assert_broadcastable/values/rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  op: &quot;NoOp&quot;\\n}\\nnode {\\n  name: &quot;hinge_loss/ToFloat_3/x&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  input: &quot;hinge_loss/ToFloat_3/x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;hinge_loss/Mul_1&quot;\\n  input: &quot;hinge_loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/Equal/y&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;hinge_loss/ToFloat_3/x&quot;\\n  input: &quot;hinge_loss/num_present/Equal/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/ones_like/Shape&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/ones_like/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/ones_like&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;hinge_loss/num_present/ones_like/Shape&quot;\\n  input: &quot;hinge_loss/num_present/ones_like/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/num_present/Equal&quot;\\n  input: &quot;hinge_loss/num_present/zeros_like&quot;\\n  input: &quot;hinge_loss/num_present/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/assert_broadcastable/weights/shape&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/assert_broadcastable/weights/rank&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/assert_broadcastable/values/shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/assert_broadcastable/values/rank&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/assert_broadcastable/static_scalar_check_success&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/ones_like/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  input: &quot;^hinge_loss/num_present/broadcast_weights/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/ones_like/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  input: &quot;^hinge_loss/num_present/broadcast_weights/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/ones_like&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights/ones_like/Shape&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights/ones_like/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/num_present/Select&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights&quot;\\n  input: &quot;hinge_loss/num_present/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Const_1&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;hinge_loss/Sum&quot;\\n  input: &quot;hinge_loss/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Greater/y&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Greater&quot;\\n  op: &quot;Greater&quot;\\n  input: &quot;hinge_loss/num_present&quot;\\n  input: &quot;hinge_loss/Greater/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Equal/y&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;hinge_loss/num_present&quot;\\n  input: &quot;hinge_loss/Equal/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/ones_like_1/Shape&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/ones_like_1/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/ones_like_1&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;hinge_loss/ones_like_1/Shape&quot;\\n  input: &quot;hinge_loss/ones_like_1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/Equal&quot;\\n  input: &quot;hinge_loss/ones_like_1&quot;\\n  input: &quot;hinge_loss/num_present&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/div&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;hinge_loss/Sum_1&quot;\\n  input: &quot;hinge_loss/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/value&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/Greater&quot;\\n  input: &quot;hinge_loss/div&quot;\\n  input: &quot;hinge_loss/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;hinge_loss/value&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/Mean_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Tile/multiples&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Mean_grad/Reshape&quot;\\n  input: &quot;gradients/Mean_grad/Tile/multiples&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Mean_grad/Shape&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/Mean_grad/Shape&quot;\\n  input: &quot;gradients/Mean_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Mean_grad/Shape&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Mean_grad/Shape&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/Mean_grad/Shape_1&quot;\\n  input: &quot;gradients/Mean_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Mean_grad/Shape&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Mean_grad/Shape&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Mean_grad/Prod_1&quot;\\n  input: &quot;gradients/Mean_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Mean_grad/Shape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Mean_grad/Prod&quot;\\n  input: &quot;gradients/Mean_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Mean_grad/Shape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/Mean_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/Mean_grad/Tile&quot;\\n  input: &quot;gradients/Mean_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/value_grad/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/value_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/Greater&quot;\\n  input: &quot;gradients/Mean_grad/truediv&quot;\\n  input: &quot;gradients/hinge_loss/value_grad/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/value_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/Greater&quot;\\n  input: &quot;gradients/hinge_loss/value_grad/zeros_like&quot;\\n  input: &quot;gradients/Mean_grad/truediv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/value_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/value_grad/Select&quot;\\n  input: &quot;^gradients/hinge_loss/value_grad/Select_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/value_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/value_grad/Select&quot;\\n  input: &quot;^gradients/hinge_loss/value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/value_grad/Select&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/value_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/value_grad/Select_1&quot;\\n  input: &quot;^gradients/hinge_loss/value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/value_grad/Select_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Shape&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/RealDiv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/hinge_loss/value_grad/tuple/control_dependency&quot;\\n  input: &quot;hinge_loss/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/RealDiv&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Sum&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;hinge_loss/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/RealDiv_1&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Neg&quot;\\n  input: &quot;hinge_loss/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/RealDiv_2&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/RealDiv_1&quot;\\n  input: &quot;hinge_loss/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/hinge_loss/value_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/RealDiv_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/mul&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Sum_1&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/div_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/div_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/div_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/div_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Reshape_1&quot;\\n  input: &quot;^gradients/hinge_loss/div_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/div_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_1_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/hinge_loss/Sum_1_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_1_grad/Tile/multiples&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/hinge_loss/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Sum_1_grad/Tile/multiples&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Select_grad/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Select_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/Equal&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/hinge_loss/Select_grad/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Select_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/Equal&quot;\\n  input: &quot;gradients/hinge_loss/Select_grad/zeros_like&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Select_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/Select_grad/Select&quot;\\n  input: &quot;^gradients/hinge_loss/Select_grad/Select_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Select_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Select_grad/Select&quot;\\n  input: &quot;^gradients/hinge_loss/Select_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Select_grad/Select&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Select_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Select_grad/Select_1&quot;\\n  input: &quot;^gradients/hinge_loss/Select_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Select_grad/Select_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Sum_1_grad/Tile&quot;\\n  input: &quot;gradients/hinge_loss/Sum_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/hinge_loss/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Sum_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Shape&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/hinge_loss/Sum_grad/Tile&quot;\\n  input: &quot;hinge_loss/ToFloat_3/x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/mul&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  input: &quot;gradients/hinge_loss/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Select_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/hinge_loss/num_present_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/hinge_loss/num_present_grad/Reshape&quot;\\n  input: &quot;gradients/hinge_loss/num_present_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Shape&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/hinge_loss/num_present_grad/Tile&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/mul&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Sum&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/num_present/Select&quot;\\n  input: &quot;gradients/hinge_loss/num_present_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/mul_1&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Sum_1&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape_1&quot;\\n  input: &quot;^gradients/hinge_loss/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights/ones_like_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights/ones_like_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights/ones_like_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Shape&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Relu_grad/ReluGrad&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Sum&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Relu_grad/ReluGrad&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Neg&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/Sub_1_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/Sub_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/Sub_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Sub_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/hinge_loss/Sub_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Sub_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Shape&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/mul&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/Sub&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/mul_1&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Sum_1&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;MatMul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 10\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/add_1_grad/Shape&quot;\\n  input: &quot;gradients/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/add_1_grad/Sum&quot;\\n  input: &quot;gradients/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;W1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;gradients/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/Reshape_grad/Reshape&quot;\\n  input: &quot;Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 32\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/add_grad/Shape&quot;\\n  input: &quot;gradients/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Relu_grad/ReluGrad&quot;\\n  input: &quot;gradients/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/add_grad/Sum&quot;\\n  input: &quot;gradients/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Relu_grad/ReluGrad&quot;\\n  input: &quot;gradients/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/add_grad/Sum_1&quot;\\n  input: &quot;gradients/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/add_grad/Reshape&quot;\\n  input: &quot;^gradients/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/add_grad/Reshape&quot;\\n  input: &quot;^gradients/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Conv2D_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Wconv1/read&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Conv2D_grad/ShapeN&quot;\\n  input: &quot;Wconv1/read&quot;\\n  input: &quot;gradients/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;gradients/Conv2D_grad/ShapeN:1&quot;\\n  input: &quot;gradients/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.00019999999494757503\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_Wconv1/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;Wconv1&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/Conv2D_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_bconv1/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;bconv1&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/add_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_W1/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;W1&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_b1/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;b1&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_Wconv1/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_bconv1/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_W1/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_b1/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Wconv1/Assign&quot;\\n  input: &quot;^bconv1/Assign&quot;\\n  input: &quot;^W1/Assign&quot;\\n  input: &quot;^b1/Assign&quot;\\n}\\nnode {\\n  name: &quot;ArgMax/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ArgMax&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;add_1&quot;\\n  input: &quot;ArgMax/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;output_type&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;ArgMax&quot;\\n  input: &quot;Placeholder_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;Equal&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Mean_1&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Cast&quot;\\n  input: &quot;Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ArgMax_1/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ArgMax_1&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;add_1&quot;\\n  input: &quot;ArgMax_1/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;output_type&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Equal_1&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;ArgMax_1&quot;\\n  input: &quot;Placeholder_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cast_1&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;Equal_1&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Mean_2&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Cast_1&quot;\\n  input: &quot;Const_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.9161997145348099&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard for Visualization\n",
    "\n",
    "Tensorflow provides a very useful tool: Tensorboard. This is very helpful to visualize the training loss, accuray, filters,...\n",
    "\n",
    "Here is a good video about Tensorboard: https://www.youtube.com/watch?v=eBbEDRsCmv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def simple_model(X,y):\n",
    "    # define our weights (e.g. init_two_layer_convnet)\n",
    "    \n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[7, 7, 3, 32])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "    W1 = tf.get_variable(\"W1\", shape=[5408, 10])\n",
    "    b1 = tf.get_variable(\"b1\", shape=[10])\n",
    "    variable_summaries(Wconv1)\n",
    "    variable_summaries(W1)\n",
    "\n",
    "    # define our graph (e.g. two_layer_convnet)\n",
    "    a1 = tf.nn.conv2d(X, Wconv1, strides=[1,2,2,1], padding='VALID') + bconv1\n",
    "    h1 = tf.nn.relu(a1)\n",
    "    h1_flat = tf.reshape(h1,[-1,5408])\n",
    "    y_out = tf.matmul(h1_flat,W1) + b1\n",
    "    return y_out\n",
    "\n",
    "y_out = simple_model(X,y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define SGD optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(2e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_with_tensorboard(session, predict, loss_value, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, tensorboard_writer=None):\n",
    "    \n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    \n",
    "    training_now = (training is not None)\n",
    "    \n",
    "    tf.summary.scalar(\"cost\", loss_value)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    # keep track of losses\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        # shuffle indicies\n",
    "        np.random.shuffle(train_indicies)\n",
    "        # keep track of accuracy\n",
    "        correct = 0\n",
    "        # make sure we iterate over the dataset once\n",
    "        batch_count = int(math.ceil(Xd.shape[0]/batch_size))\n",
    "        for i in range(batch_count):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            \n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            if training_now:\n",
    "                _, summary = session.run([training, summary_op],feed_dict=feed_dict)\n",
    "                # write log\n",
    "                tensorboard_writer.add_summary(summary, e * batch_count + i)\n",
    "            else:\n",
    "                summary = session.run(summary_op, feed_dict=feed_dict)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD optimizer\n",
      "Training\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=logs/train \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "# define SGD optimizer\n",
    "print('SGD optimizer')\n",
    "optimizer = tf.train.AdamOptimizer(1e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter('logs/train', graph=tf.get_default_graph())\n",
    "    #with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    run_model_with_tensorboard(sess,y_out,mean_loss,X_train,y_train,1,64,100,train_step,\n",
    "                               tensorboard_writer=train_writer)\n",
    "\n",
    "# tensorboard --logdir=logs/train\n",
    "print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=logs/train \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "\n",
    "# NOTE: In Window, you may not able to run the Tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules\n",
    "\n",
    "You are going to see [ADAM optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) being used. Now, we will compare the training loss curves of a model with SGD and ADAM optimizers.\n",
    "\n",
    "You can try other optimizers, e.g., [SGD+Momentum](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer), [RMSprop](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer), [Adagrad](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer), [Adadelta](https://www.tensorflow.org/api_docs/python/tf/train/AdadeltaOptimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y_out = simple_model(X,y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define SGD optimizer\n",
    "print('SGD optimizer')\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    _,_, sgd_losses = run_model(sess,y_out_deep,mean_loss,X_train,y_train,1,64,100,train_step,False)\n",
    "    print('Validation')\n",
    "    run_model(sess,y_out_deep,mean_loss,X_val,y_val,1,64)\n",
    "\n",
    "print(\"==========================================================\\n\")\n",
    "# define Adam optimizer\n",
    "print('ADAM optimizer')\n",
    "optimizer = tf.train.AdamOptimizer(1e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    _,_, adam_losses = run_model(sess,y_out_deep,mean_loss,X_train,y_train,1,64,100,train_step,False)\n",
    "    print('Validation')\n",
    "    run_model(sess,y_out_deep,mean_loss,X_val,y_val,1,64)\n",
    "    \n",
    "plt.plot(sgd_losses, label='SGD')\n",
    "plt.plot(adam_losses, label='ADAM')\n",
    "# plt.plot(adam_losses_batchnorm, label='ADAM+BatchNorm')\n",
    "# plt.ylim( (0, 100) ) \n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Epoch 1 Loss')\n",
    "plt.xlabel('minibatch number')\n",
    "plt.ylabel('minibatch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go Deeper\n",
    "\n",
    "In the previous exercises, you are required to implement different functions, e.g., affine, relu, conv2d, ... which serve as basic module to build a Deep Neuron Network. Similarly, we provide the basic modules for Tensorflow in `libs/tf_layers.py`.\n",
    "\n",
    "**NOTE:** In this exercise, you are welcome to change the block functions in `libs/tf_layers.py` to fit your needs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a deep network\n",
    "def deep_model(X, y, batchnorm=False, name=None):\n",
    "    output = Conv2D(X, 3, 7, 8, name=name+'_conv1')\n",
    "    output = tf.nn.relu(output, name=name+'_relu1')\n",
    "    if batchnorm:\n",
    "        output = BatchNormalization(output, True, name=name+'_BN1')\n",
    "    output = Conv2D(output, 8, 7, 8, name=name+'_conv2')\n",
    "    output = tf.nn.relu(output, name=name+'_relu2')\n",
    "    if batchnorm:\n",
    "        output = BatchNormalization(output, True, name=name+'_BN2')\n",
    "    output = MaxPooling2D(output, name=name+'_maxpool1')\n",
    "    output = Conv2D(output, 8, 7, 16, name=name+'_conv3')\n",
    "    output = tf.nn.relu(output, name=name+'_relu3')\n",
    "    if batchnorm:\n",
    "        output = BatchNormalization(output, True, name=name+'_BN3')\n",
    "    output = Conv2D(output, 16, 7, 16, name=name+'_conv4')\n",
    "    \n",
    "    # Here is another way of defining a name for a layer\n",
    "    with tf.variable_scope(name+'_relu4'):\n",
    "#         output = tf.nn.relu(output, name=name+'_relu4')\n",
    "        output = tf.nn.relu(output)\n",
    "    if batchnorm:\n",
    "        output = BatchNormalization(output, True, name=name+'_BN4')\n",
    "    output = MaxPooling2D(output, name=name+'_maxpool2')\n",
    "    output = tf.reshape(output, [-1, 16*8*8], name=name+'_flatten')\n",
    "    output = FullyConnected(output, 16*8*8, 100, name=name+'_fc1')\n",
    "    output = tf.nn.relu(output, name=name+'_relu5')\n",
    "    output = FullyConnected(output, 100, 100, name=name+'_fc2')\n",
    "    output = tf.nn.relu(output, name=name+'_relu6')\n",
    "    output = FullyConnected(output, 100, 10, name=name+'_fc3')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see the benefit of using **batch normalization** layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y_out_deep2 = deep_model(X,y, True, name='deep2')\n",
    "# define our loss\n",
    "total_loss_deep2 = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out_deep2)\n",
    "mean_loss = tf.reduce_mean(total_loss_deep2)\n",
    "\n",
    "print(\"==========================================================\\n\")\n",
    "# define Adam optimizer\n",
    "print('ADAM optimizer')\n",
    "optimizer = tf.train.AdamOptimizer(1e-4) # select optimizer and set learning rate\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    _,_, adam_losses_batchnorm = run_model(sess,y_out_deep2,mean_loss,X_train,y_train,1,64,100,train_step,False)\n",
    "    print('Validation')\n",
    "    run_model(sess,y_out_deep2,mean_loss,X_val,y_val,1,64)\n",
    "    \n",
    "plt.plot(sgd_losses, label='SGD')\n",
    "plt.plot(adam_losses, label='ADAM')\n",
    "plt.plot(adam_losses_batchnorm, label='ADAM+BatchNorm')\n",
    "plt.ylim( (0, 100) ) \n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Epoch 1 Loss')\n",
    "plt.xlabel('minibatch number')\n",
    "plt.ylabel('minibatch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More epochs\n",
    "Train the model with more epochs to see how good performance it can achieve.\n",
    "\n",
    "**NOTE:** If you run this on a CPU, it will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y_out_deep2 = deep_model(X,y, True, name='deep2')\n",
    "# define our loss\n",
    "total_loss_deep2 = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out_deep2)\n",
    "mean_loss = tf.reduce_mean(total_loss_deep2)\n",
    "\n",
    "print(\"==========================================================\")\n",
    "# define Adam optimizer\n",
    "print('ADAM optimizer')\n",
    "optimizer = tf.train.AdamOptimizer(1e-4) # select optimizer and set learning rate\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    _,_, adam_losses_batchnorm = run_model(sess,y_out_deep2,mean_loss,X_train,y_train,5,64,100,train_step,True)\n",
    "    print('Validation')\n",
    "    run_model(sess,y_out_deep2,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train a _GREAT_ model on CIFAR-10!\n",
    "\n",
    "### Things you should try:\n",
    "- **Filter size**: Above we used 7x7; bigger filters captures more information but smaller filters may be more computationally efficient.\n",
    "- **Number of filters**: Above we used 32 (or less) filters. Do more or fewer do better?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?\n",
    "- **Network architecture**: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Use TensorFlow Scope**: Use TensorFlow scope and/or [tf.layers](https://www.tensorflow.org/api_docs/python/tf/layers) to make it easier to write deeper networks. See [this tutorial](https://www.tensorflow.org/tutorials/layers) for how to use `tf.layers`. \n",
    "- **Use Learning Rate Decay**: [As the notes point out](http://cs231n.github.io/neural-networks-3/#anneal), decaying the learning rate might help the model converge. Feel free to decay every epoch, when loss doesn't change over an entire epoch, or any other heuristic you find appropriate. See the [Tensorflow documentation](https://www.tensorflow.org/versions/master/api_guides/python/train#Decaying_the_learning_rate) for learning rate decay.\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: Add l2 weight regularization, or perhaps use [Dropout as in the TensorFlow MNIST tutorial](https://www.tensorflow.org/get_started/mnist/pros).\n",
    "\n",
    "**NOTE:**\n",
    "* In this exercise, you are welcome to change the block functions in `libs/tf_layers.py` to fit your needs the best.\n",
    "* Softmax cross-entropy loss: [tf.losses.softmax_cross_entropy](https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/losses/softmax_cross_entropy)\n",
    "* SVM loss: [tf.losses.hinge_loss](https://www.tensorflow.org/api_docs/python/tf/losses/hinge_loss)\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should **see improvement within a few hundred iterations.**\n",
    "- Remember the **coarse-to-fine** approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should **use the validation set for hyperparameter search**, and we'll save the test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at **>= 60% accuracy on the validation set**. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. The final cell in this notebook should contain the training and validation set accuracies for your final trained network.\n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "If you do decide to implement something extra, clearly describe it in the \"Extra Credit Description\" cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feel free to play with this cell\n",
    "# You can implement the model in a seperate python file.\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    pass\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = None\n",
    "optimizer = None\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feel free to play with this cell\n",
    "# This default code creates a session\n",
    "# and trains your model for 10 epochs\n",
    "# then prints the validation set accuracy\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,10,64,100,train_step,True)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test your model here, and make sure \n",
    "# the output of this cell is the accuracy\n",
    "# of your best model on the training and val sets\n",
    "# We're looking for >= 60% accuracy on Validation\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set - DO THIS ONLY ONCE\n",
    "Now that we've gotten a result that we're happy with, we test our final model on the test set. This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test')\n",
    "run_model(sess,y_out,mean_loss,X_test,y_test,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit Description\n",
    "Briefly describe what you did here.\n",
    "\n",
    "In this cell you should also write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tell us here_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:201801-compVis]",
   "language": "python",
   "name": "conda-env-201801-compVis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
